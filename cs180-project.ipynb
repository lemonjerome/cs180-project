{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f89a00a",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "For Random Forest and \"From Scratch\" Transformers - Remove special characters and stopwords\n",
    "\n",
    "For BERT model - Remover special characters only--keep punctuatuations and capitalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80bd6ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gabrielramos/.pyenv/versions/tcfd-env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/gabrielramos/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "from collections import Counter\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import classification_report\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Use a simplified stopwords list\n",
    "stop_words = {\n",
    "    \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \n",
    "    \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \n",
    "    \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \n",
    "    \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \n",
    "    \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \n",
    "    \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \n",
    "    \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \n",
    "    \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \n",
    "    \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \n",
    "    \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"\n",
    "}\n",
    "\n",
    "def clean_text(text):\n",
    "    # Normalize smart quotes & dashes\n",
    "    text = text.replace('’', \"'\").replace('‘', \"'\").replace('–', '-').replace('—', '-')\n",
    "\n",
    "    # Remove possessive 's (e.g., Verizon's -> Verizon)\n",
    "    text = re.sub(r\"'s\\b\", \"\", text)\n",
    "\n",
    "    # Handle ampersands in proper nouns (e.g., \"Moody's & Fitch\" → \"Moody_and_Fitch\")\n",
    "    text = re.sub(r'(\\b[A-Z][a-zA-Z]*?)\\s*&\\s*([A-Z][a-zA-Z]*\\b)', r'\\1_and_\\2', text)\n",
    "\n",
    "    # Remove special characters except dash for compound words\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\- ]+\", \" \", text)\n",
    "\n",
    "    # Tokenize text into words\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "    # Lemmatize and remove stopwords\n",
    "    lemmatized = [\n",
    "        TextBlob(word).words[0].lemmatize() \n",
    "        for word in tokens if word not in stop_words and len(word) > 2\n",
    "    ]\n",
    "\n",
    "    return \" \".join(lemmatized)\n",
    "\n",
    "\n",
    "\n",
    "# Read data\n",
    "train_df = pd.read_json(\"train.json1\", lines=True)\n",
    "test_df = pd.read_csv(\"dev.csv\")\n",
    "\n",
    "# Apply the text cleaning function\n",
    "train_df[\"clean_text\"] = train_df[\"text\"].apply(clean_text)\n",
    "test_df[\"clean_text\"] = test_df[\"text\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1072202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that removes only special characters (preserves punctuation, capitalization, and stopwords)\n",
    "def clean_for_bert(text):\n",
    "    # Normalize smart quotes & dashes\n",
    "    text = text.replace('’', \"'\").replace('‘', \"'\").replace('–', '-').replace('—', '-')\n",
    "\n",
    "    # Remove possessive 's (e.g., Verizon's → Verizon)\n",
    "    text = re.sub(r\"'s\\b\", \"\", text)\n",
    "\n",
    "    # Replace ampersands in proper nouns with \"and\"\n",
    "    text = re.sub(r'(\\b[A-Z][a-zA-Z]*?)\\s*&\\s*([A-Z][a-zA-Z]*\\b)', r'\\1_and_\\2', text)\n",
    "\n",
    "    # Remove special characters except basic punctuation\n",
    "    text = re.sub(r\"[^\\w\\s.,!?;:'\\\"()\\-]\", \"\", text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "# Apply the new BERT-style cleaner\n",
    "train_df[\"bert_text\"] = train_df[\"text\"].apply(clean_for_bert)\n",
    "test_df[\"bert_text\"] = test_df[\"text\"].apply(clean_for_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d24b4d",
   "metadata": {},
   "source": [
    "# Random Forest \n",
    "Use TFID Vectorization then use Grid search to find the best n_estimators for the random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3747f9f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best n_estimators: 50\n",
      "Best cross-validated score: 0.4647503809307542\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.86      0.63        29\n",
      "           1       0.62      0.64      0.63        25\n",
      "           2       0.75      0.83      0.79       103\n",
      "           3       0.75      0.22      0.34        27\n",
      "           4       1.00      0.19      0.32        16\n",
      "\n",
      "    accuracy                           0.68       200\n",
      "   macro avg       0.72      0.55      0.54       200\n",
      "weighted avg       0.72      0.68      0.65       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF + Random Forest\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,3), max_features=3000)\n",
    "X_train_vec = vectorizer.fit_transform(train_df[\"clean_text\"])\n",
    "X_test_vec = vectorizer.transform(test_df[\"text\"])\n",
    "y_train = train_df[\"label\"]\n",
    "y_test = test_df[\"label\"]\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 50, 100, 200, 300, 400, 500]\n",
    "}\n",
    "\n",
    "# Set up the grid search\n",
    "grid_search = GridSearchCV(\n",
    "    RandomForestClassifier(class_weight='balanced', random_state=42),\n",
    "    param_grid,\n",
    "    cv=3,\n",
    "    scoring='f1_macro',  # or 'accuracy', depending on your needs\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit grid search\n",
    "grid_search.fit(X_train_vec, y_train)\n",
    "\n",
    "print(\"Best n_estimators:\", grid_search.best_params_['n_estimators'])\n",
    "print(\"Best cross-validated score:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate on test set with the best estimator\n",
    "best_rf = grid_search.best_estimator_\n",
    "y_pred = best_rf.predict(X_test_vec)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff13c7d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rf_model.joblib']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the trained Random Forest model\n",
    "joblib.dump(best_rf, \"rf_model.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ffa210",
   "metadata": {},
   "source": [
    "# \"From Scratch\" Encoder-only Transformer\n",
    "\n",
    "Use general achitecture for Encoder-only Transformer:\n",
    "- Word2Vec Encoding\n",
    "- Positional Embeddings\n",
    "- Multi-headed Self-Attention\n",
    "- Multi-layered Transformer blocks\n",
    "- Dropout regularization\n",
    "\n",
    "No Pre-training. Straight to fine tuning. Hyperparamters were adjusted to reach best results possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a7c200b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# ── 1) Prepare device ───────────────────────────────────────────────────────────\n",
    "device = torch.device(\"mps\" if torch.mps.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "X_train = train_df[\"clean_text\"]\n",
    "X_test = test_df[\"text\"]\n",
    "\n",
    "y_train = train_df[\"label\"]\n",
    "y_test = test_df[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39bbbc7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class  0:  300 samples, 23.1%\n",
      "Class  1:  255 samples, 19.6%\n",
      "Class  2:  519 samples, 39.9%\n",
      "Class  3:  164 samples, 12.6%\n",
      "Class  4:   62 samples, 4.8%\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counts = Counter(y_train.tolist())\n",
    "total = len(y_train)\n",
    "\n",
    "for label, cnt in sorted(counts.items()):\n",
    "    print(f\"Class {label:>2}: {cnt:>4} samples, {cnt/total:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cca5e6a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label‐distribution entropy ≈ 1.4308\n"
     ]
    }
   ],
   "source": [
    "counts = Counter(y_train)\n",
    "probs  = torch.tensor([counts[c]/len(y_train) for c in sorted(counts)], device=device)\n",
    "entropy = - (probs * probs.log()).sum().item()\n",
    "print(f\"Label‐distribution entropy ≈ {entropy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30d7345b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 2) Word2Vec on train+test ──────────────────────────────────────────────────\n",
    "train_texts = X_train.tolist()\n",
    "test_texts  = X_test.tolist()\n",
    "\n",
    "train_labels = y_train.tolist()\n",
    "test_labels  = y_test.tolist()\n",
    "\n",
    "tokenized_train = [t.split() for t in train_texts]\n",
    "tokenized_test  = [t.split() for t in test_texts]\n",
    "all_tokenized   = tokenized_train + tokenized_test\n",
    "\n",
    "dimensions = 16\n",
    "attn_heads = 2\n",
    "ffn_layer_dimensions = 32\n",
    "num_layers = 2\n",
    "\n",
    "w2v = Word2Vec(\n",
    "    sentences=all_tokenized,\n",
    "    vector_size=dimensions,\n",
    "    window=5,\n",
    "    min_count=3,\n",
    "    workers=8\n",
    ")\n",
    "\n",
    "def get_embedding(word):\n",
    "    # fallback to zero-vector if OOV\n",
    "    return w2v.wv[word] if word in w2v.wv else np.zeros(dimensions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bea3f000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95th percentile train length: 87\n",
      "95th percentile test length:  151\n",
      "96th percentile train length: 93\n",
      "96th percentile test length:  164\n",
      "97th percentile train length: 100\n",
      "97th percentile test length:  178\n",
      "98th percentile train length: 110\n",
      "98th percentile test length:  208\n",
      "99th percentile train length: 151\n",
      "99th percentile test length:  240\n"
     ]
    }
   ],
   "source": [
    "# Get max_length\n",
    "# Tokenize\n",
    "tok_train = [txt.split() for txt in X_train]\n",
    "tok_test  = [txt.split() for txt in X_test]\n",
    "\n",
    "# Compute lengths\n",
    "train_lens = np.array([len(s) for s in tok_train])\n",
    "test_lens  = np.array([len(s) for s in tok_test])\n",
    "\n",
    "# Decide your percentile, e.g. 95%\n",
    "for p in range(95, 100):\n",
    "    max_len_train = int(np.percentile(train_lens, p))\n",
    "    max_len_test  = int(np.percentile(test_lens,  p))\n",
    "\n",
    "    print(f\"{p}th percentile train length: {max_len_train}\")\n",
    "    print(f\"{p}th percentile test length:  {max_len_test}\")\n",
    "\n",
    "max_len = 128  # Set this to the maximum length you want to use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc40ce24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 3) Positional encoding helper ──────────────────────────────────────────────\n",
    "def positional_encoding(seq_len, d_model):\n",
    "    pos = np.arange(seq_len)[:, None]\n",
    "    i   = np.arange(d_model)[None, :]\n",
    "    angle_rates = 1 / np.power(10000, (2*(i//2)) / np.float32(d_model))\n",
    "    angles = pos * angle_rates\n",
    "    angles[:, 0::2] = np.sin(angles[:, 0::2])\n",
    "    angles[:, 1::2] = np.cos(angles[:, 1::2])\n",
    "    return torch.from_numpy(angles).float()  # [seq_len, d_model]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0be53494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 4) Dataset + Dataloader ───────────────────────────────────────────────────\n",
    "class TCfDDataset(Dataset):\n",
    "    def __init__(self, texts, labels, max_len=max_len):\n",
    "        self.texts   = texts\n",
    "        self.labels  = labels\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.texts[idx].split()[:self.max_len]\n",
    "        embs   = [get_embedding(t) for t in tokens]\n",
    "        # pad to max_len\n",
    "        if len(embs) < self.max_len:\n",
    "            pad = [np.zeros(dimensions)] * (self.max_len - len(embs))\n",
    "            embs.extend(pad)\n",
    "        x = torch.tensor(np.stack(embs), dtype=torch.float32)\n",
    "        y = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "128ff2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Prepare your two datasets *without* splitting train_df\n",
    "train_ds = TCfDDataset(\n",
    "    texts = train_texts,\n",
    "    labels= train_labels,\n",
    "    max_len = max_len\n",
    ")\n",
    "\n",
    "test_ds  = TCfDDataset(\n",
    "    texts = test_texts,\n",
    "    labels= test_labels,\n",
    "    max_len = max_len\n",
    ")\n",
    "\n",
    "# 1) Count labels\n",
    "counts = Counter(train_labels)            # e.g. {0:50, 1:10, 2:5, ...}\n",
    "\n",
    "# 2) Compute per-example weight = 1 / count[label]\n",
    "example_weights = [1.0 / counts[label] for label in train_labels]\n",
    "example_weights = torch.tensor(example_weights, dtype=torch.double)\n",
    "\n",
    "# 3) Create the sampler\n",
    "sampler = WeightedRandomSampler(\n",
    "    weights=example_weights,    # a list/1D tensor of length N_train\n",
    "    num_samples=len(example_weights),  # draw this many samples per epoch\n",
    "    replacement=True            # sample with replacement\n",
    ")\n",
    "\n",
    "# 4) DataLoader with sampler (no shuffle!)\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=512,\n",
    "    sampler=sampler,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "test_loader  = DataLoader(test_ds,  batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8079d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 5) Model ───────────────────────────────────────────────────────────────────\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, heads, ff_hidden_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.attn   = nn.MultiheadAttention(embed_dim, heads, dropout=dropout, batch_first=True)\n",
    "        self.norm1  = nn.LayerNorm(embed_dim)\n",
    "        self.norm2  = nn.LayerNorm(embed_dim)\n",
    "        self.ff     = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_hidden_dim, embed_dim)\n",
    "        )\n",
    "        self.drop   = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_out, _ = self.attn(x, x, x)\n",
    "        x = self.norm1(x + self.drop(attn_out))\n",
    "        ff = self.ff(x)\n",
    "        return self.norm2(x + self.drop(ff))\n",
    "\n",
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, embed_dim, heads, ff_hidden, num_classes, dropout, max_len, num_layers):\n",
    "        super().__init__()\n",
    "        self.max_len   = max_len\n",
    "        self.embed_dim = embed_dim\n",
    "        pe = positional_encoding(max_len, embed_dim)  # [max_len, embed_dim]\n",
    "        self.register_buffer(\"pos_enc\", pe)           # moves with model.to(device)\n",
    "        # create N identical blocks\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderBlock(embed_dim, heads, ff_hidden, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.classifier = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, L, D]\n",
    "        L = x.size(1)\n",
    "        x = x + self.pos_enc[:L, :].unsqueeze(0)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = x.mean(1)\n",
    "        return self.classifier(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60ab61ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextClassifier(\n",
    "    embed_dim=dimensions,\n",
    "    heads=attn_heads,\n",
    "    ff_hidden=ffn_layer_dimensions,\n",
    "    num_classes=len(np.unique(y_train)),\n",
    "    dropout=0.5,\n",
    "    max_len=max_len,\n",
    "    num_layers=num_layers\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=11e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72d38537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 — train loss: 1.6669\n",
      "Epoch 02 — train loss: 1.6591\n",
      "Epoch 03 — train loss: 1.6134\n",
      "Epoch 04 — train loss: 1.6128\n",
      "Epoch 05 — train loss: 1.6099\n",
      "Epoch 06 — train loss: 1.6031\n",
      "Epoch 07 — train loss: 1.5986\n",
      "Epoch 08 — train loss: 1.6041\n",
      "Epoch 09 — train loss: 1.6015\n",
      "Epoch 10 — train loss: 1.5969\n",
      "Epoch 11 — train loss: 1.5871\n",
      "Epoch 12 — train loss: 1.5974\n",
      "Epoch 13 — train loss: 1.5771\n",
      "Epoch 14 — train loss: 1.5721\n",
      "Epoch 15 — train loss: 1.5761\n",
      "Epoch 16 — train loss: 1.5612\n",
      "Epoch 17 — train loss: 1.5714\n",
      "Epoch 18 — train loss: 1.5720\n",
      "Epoch 19 — train loss: 1.5389\n",
      "Epoch 20 — train loss: 1.5615\n",
      "Epoch 21 — train loss: 1.5084\n",
      "Epoch 22 — train loss: 1.4703\n",
      "Epoch 23 — train loss: 1.4573\n",
      "Epoch 24 — train loss: 1.4504\n",
      "Epoch 25 — train loss: 1.4441\n",
      "Epoch 26 — train loss: 1.4212\n",
      "Epoch 27 — train loss: 1.4342\n",
      "Epoch 28 — train loss: 1.4388\n",
      "Epoch 29 — train loss: 1.3934\n",
      "Epoch 30 — train loss: 1.4287\n",
      "Epoch 31 — train loss: 1.4444\n",
      "Epoch 32 — train loss: 1.4360\n",
      "Epoch 33 — train loss: 1.3878\n",
      "Epoch 34 — train loss: 1.3768\n",
      "Epoch 35 — train loss: 1.4031\n",
      "Epoch 36 — train loss: 1.3975\n",
      "Epoch 37 — train loss: 1.4611\n",
      "Epoch 38 — train loss: 1.4632\n",
      "Epoch 39 — train loss: 1.4012\n",
      "Epoch 40 — train loss: 1.4080\n",
      "Epoch 41 — train loss: 1.4219\n",
      "Epoch 42 — train loss: 1.3816\n",
      "Epoch 43 — train loss: 1.3599\n",
      "Epoch 44 — train loss: 1.3545\n",
      "Epoch 45 — train loss: 1.3597\n",
      "Epoch 46 — train loss: 1.3510\n",
      "Epoch 47 — train loss: 1.3694\n",
      "Epoch 48 — train loss: 1.3749\n",
      "Epoch 49 — train loss: 1.3810\n",
      "Epoch 50 — train loss: 1.3626\n",
      "Epoch 51 — train loss: 1.3381\n",
      "Epoch 52 — train loss: 1.3867\n",
      "Epoch 53 — train loss: 1.3781\n",
      "Epoch 54 — train loss: 1.3784\n",
      "Epoch 55 — train loss: 1.3394\n",
      "Epoch 56 — train loss: 1.3592\n",
      "Epoch 57 — train loss: 1.3858\n",
      "Epoch 58 — train loss: 1.3939\n",
      "Epoch 59 — train loss: 1.3885\n",
      "Epoch 60 — train loss: 1.3942\n",
      "Epoch 61 — train loss: 1.3577\n",
      "Epoch 62 — train loss: 1.3461\n",
      "Epoch 63 — train loss: 1.3504\n",
      "Epoch 64 — train loss: 1.3174\n",
      "Epoch 65 — train loss: 1.3069\n",
      "Epoch 66 — train loss: 1.3114\n",
      "Epoch 67 — train loss: 1.3272\n",
      "Epoch 68 — train loss: 1.2974\n",
      "Epoch 69 — train loss: 1.2994\n",
      "Epoch 70 — train loss: 1.2843\n",
      "Epoch 71 — train loss: 1.3123\n",
      "Epoch 72 — train loss: 1.3050\n",
      "Epoch 73 — train loss: 1.3666\n",
      "Epoch 74 — train loss: 1.3799\n",
      "Epoch 75 — train loss: 1.2912\n",
      "Epoch 76 — train loss: 1.3291\n",
      "Epoch 77 — train loss: 1.2918\n",
      "Epoch 78 — train loss: 1.3112\n",
      "Epoch 79 — train loss: 1.2834\n",
      "Epoch 80 — train loss: 1.2673\n",
      "Epoch 81 — train loss: 1.3675\n",
      "Epoch 82 — train loss: 1.3734\n",
      "Epoch 83 — train loss: 1.3064\n",
      "Epoch 84 — train loss: 1.3158\n",
      "Epoch 85 — train loss: 1.3207\n",
      "Epoch 86 — train loss: 1.2739\n",
      "Epoch 87 — train loss: 1.3119\n",
      "Epoch 88 — train loss: 1.2901\n",
      "Epoch 89 — train loss: 1.2742\n",
      "Epoch 90 — train loss: 1.2581\n",
      "Epoch 91 — train loss: 1.3113\n",
      "Epoch 92 — train loss: 1.2849\n",
      "Epoch 93 — train loss: 1.2446\n",
      "Epoch 94 — train loss: 1.2232\n",
      "Epoch 95 — train loss: 1.2114\n",
      "Epoch 96 — train loss: 1.2284\n",
      "Epoch 97 — train loss: 1.2520\n",
      "Epoch 98 — train loss: 1.2525\n",
      "Epoch 99 — train loss: 1.2268\n",
      "Epoch 100 — train loss: 1.2014\n",
      "Epoch 101 — train loss: 1.2636\n",
      "Epoch 102 — train loss: 1.2229\n",
      "Epoch 103 — train loss: 1.2345\n",
      "Epoch 104 — train loss: 1.1841\n",
      "Epoch 105 — train loss: 1.1886\n",
      "Epoch 106 — train loss: 1.1850\n",
      "Epoch 107 — train loss: 1.1790\n",
      "Epoch 108 — train loss: 1.2106\n",
      "Epoch 109 — train loss: 1.1902\n",
      "Epoch 110 — train loss: 1.2343\n",
      "Epoch 111 — train loss: 1.2225\n",
      "Epoch 112 — train loss: 1.1904\n",
      "Epoch 113 — train loss: 1.1727\n",
      "Epoch 114 — train loss: 1.1487\n",
      "Epoch 115 — train loss: 1.1876\n",
      "Epoch 116 — train loss: 1.1608\n",
      "Epoch 117 — train loss: 1.1607\n",
      "Epoch 118 — train loss: 1.1421\n",
      "Epoch 119 — train loss: 1.1581\n",
      "Epoch 120 — train loss: 1.1689\n",
      "Epoch 121 — train loss: 1.1557\n",
      "Epoch 122 — train loss: 1.1908\n",
      "Epoch 123 — train loss: 1.1558\n",
      "Epoch 124 — train loss: 1.1412\n",
      "Epoch 125 — train loss: 1.1341\n",
      "Epoch 126 — train loss: 1.1296\n",
      "Epoch 127 — train loss: 1.1307\n",
      "Epoch 128 — train loss: 1.1386\n",
      "Epoch 129 — train loss: 1.1088\n",
      "Epoch 130 — train loss: 1.0906\n",
      "Epoch 131 — train loss: 1.1150\n",
      "Epoch 132 — train loss: 1.1005\n",
      "Epoch 133 — train loss: 1.1102\n",
      "Epoch 134 — train loss: 1.1617\n",
      "Epoch 135 — train loss: 1.1739\n",
      "Epoch 136 — train loss: 1.1492\n",
      "Epoch 137 — train loss: 1.1328\n",
      "Epoch 138 — train loss: 1.1029\n",
      "Epoch 139 — train loss: 1.1798\n",
      "Epoch 140 — train loss: 1.1241\n",
      "Epoch 141 — train loss: 1.1352\n",
      "Epoch 142 — train loss: 1.1069\n",
      "Epoch 143 — train loss: 1.0908\n",
      "Epoch 144 — train loss: 1.0494\n",
      "Epoch 145 — train loss: 1.0456\n",
      "Epoch 146 — train loss: 1.0833\n",
      "Epoch 147 — train loss: 1.0910\n",
      "Epoch 148 — train loss: 1.0613\n",
      "Epoch 149 — train loss: 1.0771\n",
      "Epoch 150 — train loss: 1.0944\n",
      "Epoch 151 — train loss: 1.1108\n",
      "Epoch 152 — train loss: 1.0479\n",
      "Epoch 153 — train loss: 1.0922\n",
      "Epoch 154 — train loss: 1.1030\n",
      "Epoch 155 — train loss: 1.0803\n",
      "Epoch 156 — train loss: 1.0989\n",
      "Epoch 157 — train loss: 1.0872\n",
      "Epoch 158 — train loss: 1.0861\n",
      "Epoch 159 — train loss: 1.0669\n",
      "Epoch 160 — train loss: 1.0688\n",
      "Epoch 161 — train loss: 1.0990\n",
      "Epoch 162 — train loss: 1.0429\n",
      "Epoch 163 — train loss: 1.0853\n",
      "Epoch 164 — train loss: 1.0623\n",
      "Epoch 165 — train loss: 1.0786\n",
      "Epoch 166 — train loss: 1.0553\n",
      "Epoch 167 — train loss: 1.0176\n",
      "Epoch 168 — train loss: 1.0909\n",
      "Epoch 169 — train loss: 1.0855\n",
      "Epoch 170 — train loss: 1.0717\n",
      "Epoch 171 — train loss: 1.0376\n",
      "Epoch 172 — train loss: 0.9854\n",
      "Epoch 173 — train loss: 0.9893\n",
      "Epoch 174 — train loss: 0.9983\n",
      "Epoch 175 — train loss: 1.0450\n",
      "Epoch 176 — train loss: 1.0456\n",
      "Epoch 177 — train loss: 1.0024\n",
      "Epoch 178 — train loss: 0.9937\n",
      "Epoch 179 — train loss: 0.9580\n",
      "Epoch 180 — train loss: 1.0015\n",
      "Epoch 181 — train loss: 1.0127\n",
      "Epoch 182 — train loss: 1.0139\n",
      "Epoch 183 — train loss: 1.0438\n",
      "Epoch 184 — train loss: 1.0478\n",
      "Epoch 185 — train loss: 1.0161\n",
      "Epoch 186 — train loss: 0.9908\n",
      "Epoch 187 — train loss: 1.0099\n",
      "Epoch 188 — train loss: 1.0128\n",
      "Epoch 189 — train loss: 1.0139\n",
      "Epoch 190 — train loss: 1.0784\n",
      "Epoch 191 — train loss: 1.0471\n",
      "Epoch 192 — train loss: 1.0353\n",
      "Epoch 193 — train loss: 0.9709\n",
      "Epoch 194 — train loss: 0.9775\n",
      "Epoch 195 — train loss: 0.9780\n",
      "Epoch 196 — train loss: 0.9819\n",
      "Epoch 197 — train loss: 0.9492\n",
      "Epoch 198 — train loss: 0.9388\n",
      "Epoch 199 — train loss: 0.9608\n",
      "Epoch 200 — train loss: 0.9165\n",
      "Epoch 201 — train loss: 0.9545\n",
      "Epoch 202 — train loss: 0.9220\n",
      "Epoch 203 — train loss: 0.9510\n",
      "Epoch 204 — train loss: 0.9905\n",
      "Epoch 205 — train loss: 0.9557\n",
      "Epoch 206 — train loss: 0.9861\n",
      "Epoch 207 — train loss: 0.9050\n",
      "Epoch 208 — train loss: 0.9934\n",
      "Epoch 209 — train loss: 0.9803\n",
      "Epoch 210 — train loss: 0.9465\n",
      "Epoch 211 — train loss: 0.9831\n",
      "Epoch 212 — train loss: 0.9511\n",
      "Epoch 213 — train loss: 0.9517\n",
      "Epoch 214 — train loss: 0.9987\n",
      "Epoch 215 — train loss: 0.9223\n",
      "Epoch 216 — train loss: 1.0104\n",
      "Epoch 217 — train loss: 1.0079\n",
      "Epoch 218 — train loss: 0.9782\n",
      "Epoch 219 — train loss: 0.9413\n",
      "Epoch 220 — train loss: 0.9532\n",
      "Epoch 221 — train loss: 0.9391\n",
      "Epoch 222 — train loss: 0.9893\n",
      "Epoch 223 — train loss: 0.9429\n",
      "Epoch 224 — train loss: 0.9628\n",
      "Epoch 225 — train loss: 1.0595\n",
      "Epoch 226 — train loss: 1.0483\n",
      "Epoch 227 — train loss: 0.9995\n",
      "Epoch 228 — train loss: 1.0515\n",
      "Epoch 229 — train loss: 0.9906\n",
      "Epoch 230 — train loss: 0.9808\n",
      "Epoch 231 — train loss: 0.9634\n",
      "Epoch 232 — train loss: 0.9258\n",
      "Epoch 233 — train loss: 0.9541\n",
      "Epoch 234 — train loss: 0.9265\n",
      "Epoch 235 — train loss: 0.9456\n",
      "Epoch 236 — train loss: 0.9337\n",
      "Epoch 237 — train loss: 0.9821\n",
      "Epoch 238 — train loss: 0.9056\n",
      "Epoch 239 — train loss: 0.9050\n",
      "Epoch 240 — train loss: 0.8899\n",
      "Epoch 241 — train loss: 0.9084\n",
      "Epoch 242 — train loss: 0.9558\n",
      "Epoch 243 — train loss: 0.9827\n",
      "Epoch 244 — train loss: 0.9393\n",
      "Epoch 245 — train loss: 0.9391\n",
      "Epoch 246 — train loss: 0.9891\n",
      "Epoch 247 — train loss: 0.8766\n",
      "Epoch 248 — train loss: 0.9287\n",
      "Epoch 249 — train loss: 0.8779\n",
      "Epoch 250 — train loss: 0.8956\n",
      "Epoch 251 — train loss: 0.8938\n",
      "Epoch 252 — train loss: 0.9439\n",
      "Epoch 253 — train loss: 0.8857\n",
      "Epoch 254 — train loss: 0.8954\n",
      "Epoch 255 — train loss: 0.8904\n",
      "Epoch 256 — train loss: 0.8977\n",
      "Epoch 257 — train loss: 0.9497\n",
      "Epoch 258 — train loss: 0.8857\n",
      "Epoch 259 — train loss: 0.8712\n",
      "Epoch 260 — train loss: 0.8874\n",
      "Epoch 261 — train loss: 0.8565\n",
      "Epoch 262 — train loss: 0.8434\n",
      "Epoch 263 — train loss: 0.9193\n",
      "Epoch 264 — train loss: 0.9256\n",
      "Epoch 265 — train loss: 0.8869\n",
      "Epoch 266 — train loss: 0.9340\n",
      "Epoch 267 — train loss: 0.9273\n",
      "Epoch 268 — train loss: 0.8744\n",
      "Epoch 269 — train loss: 0.8443\n",
      "Epoch 270 — train loss: 0.8802\n",
      "Epoch 271 — train loss: 0.8990\n",
      "Epoch 272 — train loss: 0.8950\n",
      "Epoch 273 — train loss: 0.9085\n",
      "Epoch 274 — train loss: 0.9133\n",
      "Epoch 275 — train loss: 0.8957\n",
      "Epoch 276 — train loss: 0.8951\n",
      "Epoch 277 — train loss: 0.8633\n",
      "Epoch 278 — train loss: 0.8751\n",
      "Epoch 279 — train loss: 0.8798\n",
      "Epoch 280 — train loss: 0.8533\n",
      "Epoch 281 — train loss: 0.8312\n",
      "Epoch 282 — train loss: 0.8286\n",
      "Epoch 283 — train loss: 0.9627\n",
      "Epoch 284 — train loss: 0.9173\n",
      "Epoch 285 — train loss: 0.8658\n",
      "Epoch 286 — train loss: 0.9072\n",
      "Epoch 287 — train loss: 0.9076\n",
      "Epoch 288 — train loss: 0.9031\n",
      "Epoch 289 — train loss: 0.8799\n",
      "Epoch 290 — train loss: 0.8873\n",
      "Epoch 291 — train loss: 0.8589\n",
      "Epoch 292 — train loss: 0.8198\n",
      "Epoch 293 — train loss: 0.8198\n",
      "Epoch 294 — train loss: 0.8235\n",
      "Epoch 295 — train loss: 0.8646\n",
      "Epoch 296 — train loss: 0.8870\n",
      "Epoch 297 — train loss: 0.8872\n",
      "Epoch 298 — train loss: 0.8458\n",
      "Epoch 299 — train loss: 0.8506\n",
      "Epoch 300 — train loss: 0.8796\n",
      "Epoch 301 — train loss: 0.8650\n",
      "Epoch 302 — train loss: 0.8466\n",
      "Epoch 303 — train loss: 0.8095\n",
      "Epoch 304 — train loss: 0.8260\n",
      "Epoch 305 — train loss: 0.8689\n",
      "Epoch 306 — train loss: 0.8167\n",
      "Epoch 307 — train loss: 0.8920\n",
      "Epoch 308 — train loss: 0.9117\n",
      "Epoch 309 — train loss: 0.8938\n",
      "Epoch 310 — train loss: 0.9035\n",
      "Epoch 311 — train loss: 0.9162\n",
      "Epoch 312 — train loss: 0.9748\n",
      "Epoch 313 — train loss: 0.9132\n",
      "Epoch 314 — train loss: 0.9637\n",
      "Epoch 315 — train loss: 0.9896\n",
      "Epoch 316 — train loss: 0.8972\n",
      "Epoch 317 — train loss: 0.8861\n",
      "Epoch 318 — train loss: 0.8361\n",
      "Epoch 319 — train loss: 0.8516\n",
      "Epoch 320 — train loss: 0.8686\n",
      "Epoch 321 — train loss: 0.8561\n",
      "Epoch 322 — train loss: 0.8061\n",
      "Epoch 323 — train loss: 0.8181\n",
      "Epoch 324 — train loss: 0.8040\n",
      "Epoch 325 — train loss: 0.8174\n",
      "Epoch 326 — train loss: 0.7829\n",
      "Epoch 327 — train loss: 0.7908\n",
      "Epoch 328 — train loss: 0.7938\n",
      "Epoch 329 — train loss: 0.8253\n",
      "Epoch 330 — train loss: 0.8272\n",
      "Epoch 331 — train loss: 0.7978\n",
      "Epoch 332 — train loss: 0.7626\n",
      "Epoch 333 — train loss: 0.8146\n",
      "Epoch 334 — train loss: 0.8465\n",
      "Epoch 335 — train loss: 0.8395\n",
      "Epoch 336 — train loss: 0.8424\n",
      "Epoch 337 — train loss: 0.8294\n",
      "Epoch 338 — train loss: 0.8914\n",
      "Epoch 339 — train loss: 0.9124\n",
      "Epoch 340 — train loss: 0.8666\n",
      "Epoch 341 — train loss: 0.8773\n",
      "Epoch 342 — train loss: 0.8619\n",
      "Epoch 343 — train loss: 0.8792\n",
      "Epoch 344 — train loss: 0.9168\n",
      "Epoch 345 — train loss: 0.8669\n",
      "Epoch 346 — train loss: 0.8773\n",
      "Epoch 347 — train loss: 0.8715\n",
      "Epoch 348 — train loss: 0.8680\n",
      "Epoch 349 — train loss: 0.8653\n",
      "Epoch 350 — train loss: 0.8268\n",
      "Epoch 351 — train loss: 0.8231\n",
      "Epoch 352 — train loss: 0.8013\n",
      "Epoch 353 — train loss: 0.8258\n",
      "Epoch 354 — train loss: 0.8053\n",
      "Epoch 355 — train loss: 0.8112\n",
      "Epoch 356 — train loss: 0.8270\n",
      "Epoch 357 — train loss: 0.8747\n",
      "Epoch 358 — train loss: 0.8541\n",
      "Epoch 359 — train loss: 0.8470\n",
      "Epoch 360 — train loss: 0.8060\n",
      "Epoch 361 — train loss: 0.8264\n",
      "Epoch 362 — train loss: 0.8339\n",
      "Epoch 363 — train loss: 0.7863\n",
      "Epoch 364 — train loss: 0.8146\n",
      "Epoch 365 — train loss: 0.7407\n",
      "Epoch 366 — train loss: 0.7757\n",
      "Epoch 367 — train loss: 0.7491\n",
      "Epoch 368 — train loss: 0.7835\n",
      "Epoch 369 — train loss: 0.7894\n",
      "Epoch 370 — train loss: 0.7891\n",
      "Epoch 371 — train loss: 0.7909\n",
      "Epoch 372 — train loss: 0.8537\n",
      "Epoch 373 — train loss: 0.8339\n",
      "Epoch 374 — train loss: 0.8465\n",
      "Epoch 375 — train loss: 0.7964\n",
      "Epoch 376 — train loss: 0.8366\n",
      "Epoch 377 — train loss: 0.8060\n",
      "Epoch 378 — train loss: 0.7938\n",
      "Epoch 379 — train loss: 0.8207\n",
      "Epoch 380 — train loss: 0.8024\n",
      "Epoch 381 — train loss: 0.8043\n",
      "Epoch 382 — train loss: 0.8009\n",
      "Epoch 383 — train loss: 0.7904\n",
      "Epoch 384 — train loss: 0.8355\n",
      "Epoch 385 — train loss: 0.8069\n",
      "Epoch 386 — train loss: 0.7955\n",
      "Epoch 387 — train loss: 0.8195\n",
      "Epoch 388 — train loss: 0.8190\n",
      "Epoch 389 — train loss: 0.7531\n",
      "Epoch 390 — train loss: 0.7934\n",
      "Epoch 391 — train loss: 0.8153\n",
      "Epoch 392 — train loss: 0.8219\n",
      "Epoch 393 — train loss: 0.7981\n",
      "Epoch 394 — train loss: 0.8075\n",
      "Epoch 395 — train loss: 0.7970\n",
      "Epoch 396 — train loss: 0.8003\n",
      "Epoch 397 — train loss: 0.7998\n",
      "Epoch 398 — train loss: 0.8525\n",
      "Epoch 399 — train loss: 0.7551\n",
      "Epoch 400 — train loss: 0.7843\n",
      "Epoch 401 — train loss: 0.7436\n",
      "Epoch 402 — train loss: 0.7522\n",
      "Epoch 403 — train loss: 0.7299\n",
      "Epoch 404 — train loss: 0.7971\n",
      "Epoch 405 — train loss: 0.7597\n",
      "Epoch 406 — train loss: 0.7570\n",
      "Epoch 407 — train loss: 0.7606\n",
      "Epoch 408 — train loss: 0.7573\n",
      "Epoch 409 — train loss: 0.7314\n",
      "Epoch 410 — train loss: 0.8493\n",
      "Epoch 411 — train loss: 0.7931\n",
      "Epoch 412 — train loss: 0.8255\n",
      "Epoch 413 — train loss: 0.8198\n",
      "Epoch 414 — train loss: 0.8243\n",
      "Epoch 415 — train loss: 0.8205\n",
      "Epoch 416 — train loss: 0.8185\n",
      "Epoch 417 — train loss: 0.8250\n",
      "Epoch 418 — train loss: 0.8470\n",
      "Epoch 419 — train loss: 0.8087\n",
      "Epoch 420 — train loss: 0.8296\n",
      "Epoch 421 — train loss: 0.7594\n",
      "Epoch 422 — train loss: 0.7611\n",
      "Epoch 423 — train loss: 0.7809\n",
      "Epoch 424 — train loss: 0.8102\n",
      "Epoch 425 — train loss: 0.7438\n",
      "Epoch 426 — train loss: 0.8578\n",
      "Epoch 427 — train loss: 0.7768\n",
      "Epoch 428 — train loss: 0.7822\n",
      "Epoch 429 — train loss: 0.8044\n",
      "Epoch 430 — train loss: 0.8232\n",
      "Epoch 431 — train loss: 0.7883\n",
      "Epoch 432 — train loss: 0.7585\n",
      "Epoch 433 — train loss: 0.7663\n",
      "Epoch 434 — train loss: 0.8093\n",
      "Epoch 435 — train loss: 0.8032\n",
      "Epoch 436 — train loss: 0.7740\n",
      "Epoch 437 — train loss: 0.8121\n",
      "Epoch 438 — train loss: 0.7495\n",
      "Epoch 439 — train loss: 0.8079\n",
      "Epoch 440 — train loss: 0.7827\n",
      "Epoch 441 — train loss: 0.8093\n",
      "Epoch 442 — train loss: 0.8306\n",
      "Epoch 443 — train loss: 0.7608\n",
      "Epoch 444 — train loss: 0.7752\n",
      "Epoch 445 — train loss: 0.8143\n",
      "Epoch 446 — train loss: 0.8390\n",
      "Epoch 447 — train loss: 0.7947\n",
      "Epoch 448 — train loss: 0.7479\n",
      "Epoch 449 — train loss: 0.7521\n",
      "Epoch 450 — train loss: 0.7581\n",
      "Epoch 451 — train loss: 0.7960\n",
      "Epoch 452 — train loss: 0.7462\n",
      "Epoch 453 — train loss: 0.7634\n",
      "Epoch 454 — train loss: 0.7475\n",
      "Epoch 455 — train loss: 0.7036\n",
      "Epoch 456 — train loss: 0.7605\n",
      "Epoch 457 — train loss: 0.7696\n",
      "Epoch 458 — train loss: 0.7711\n",
      "Epoch 459 — train loss: 0.7365\n",
      "Epoch 460 — train loss: 0.7368\n",
      "Epoch 461 — train loss: 0.7951\n",
      "Epoch 462 — train loss: 0.7968\n",
      "Epoch 463 — train loss: 0.7650\n",
      "Epoch 464 — train loss: 0.7885\n",
      "Epoch 465 — train loss: 0.7793\n",
      "Epoch 466 — train loss: 0.7744\n",
      "Epoch 467 — train loss: 0.8043\n",
      "Epoch 468 — train loss: 0.7578\n",
      "Epoch 469 — train loss: 0.7719\n",
      "Epoch 470 — train loss: 0.7256\n",
      "Epoch 471 — train loss: 0.8210\n",
      "Epoch 472 — train loss: 0.7598\n",
      "Epoch 473 — train loss: 0.7291\n",
      "Epoch 474 — train loss: 0.7654\n",
      "Epoch 475 — train loss: 0.7793\n",
      "Epoch 476 — train loss: 0.8073\n",
      "Epoch 477 — train loss: 0.7649\n",
      "Epoch 478 — train loss: 0.7560\n",
      "Epoch 479 — train loss: 0.7158\n",
      "Epoch 480 — train loss: 0.7400\n",
      "Epoch 481 — train loss: 0.7316\n",
      "Epoch 482 — train loss: 0.7086\n",
      "Epoch 483 — train loss: 0.7607\n",
      "Epoch 484 — train loss: 0.6708\n",
      "Epoch 485 — train loss: 0.7399\n",
      "Epoch 486 — train loss: 0.7366\n",
      "Epoch 487 — train loss: 0.7259\n",
      "Epoch 488 — train loss: 0.7796\n",
      "Epoch 489 — train loss: 0.6980\n",
      "Epoch 490 — train loss: 0.7371\n",
      "Epoch 491 — train loss: 0.7489\n",
      "Epoch 492 — train loss: 0.7649\n",
      "Epoch 493 — train loss: 0.8080\n",
      "Epoch 494 — train loss: 0.7251\n",
      "Epoch 495 — train loss: 0.7510\n",
      "Epoch 496 — train loss: 0.7472\n",
      "Epoch 497 — train loss: 0.7119\n",
      "Epoch 498 — train loss: 0.7697\n",
      "Epoch 499 — train loss: 0.6976\n",
      "Epoch 500 — train loss: 0.7515\n",
      "Epoch 501 — train loss: 0.7540\n",
      "Epoch 502 — train loss: 0.6886\n",
      "Epoch 503 — train loss: 0.7150\n",
      "Epoch 504 — train loss: 0.7403\n",
      "Epoch 505 — train loss: 0.7256\n",
      "Epoch 506 — train loss: 0.7365\n",
      "Epoch 507 — train loss: 0.6823\n",
      "Epoch 508 — train loss: 0.7096\n",
      "Epoch 509 — train loss: 0.7375\n",
      "Epoch 510 — train loss: 0.7038\n",
      "Epoch 511 — train loss: 0.7601\n",
      "Epoch 512 — train loss: 0.8978\n",
      "Epoch 513 — train loss: 0.7581\n",
      "Epoch 514 — train loss: 0.7695\n",
      "Epoch 515 — train loss: 0.7817\n",
      "Epoch 516 — train loss: 0.7233\n",
      "Epoch 517 — train loss: 0.7445\n",
      "Epoch 518 — train loss: 0.7553\n",
      "Epoch 519 — train loss: 0.7331\n",
      "Epoch 520 — train loss: 0.7988\n",
      "Epoch 521 — train loss: 0.8018\n",
      "Epoch 522 — train loss: 0.8519\n",
      "Epoch 523 — train loss: 0.7568\n",
      "Epoch 524 — train loss: 0.7352\n",
      "Epoch 525 — train loss: 0.7188\n",
      "Epoch 526 — train loss: 0.7429\n",
      "Epoch 527 — train loss: 0.7469\n",
      "Epoch 528 — train loss: 0.7341\n",
      "Epoch 529 — train loss: 0.6651\n",
      "Epoch 530 — train loss: 0.6752\n",
      "Epoch 531 — train loss: 0.6902\n",
      "Epoch 532 — train loss: 0.7283\n",
      "Epoch 533 — train loss: 0.7276\n",
      "Epoch 534 — train loss: 0.7498\n",
      "Epoch 535 — train loss: 0.7130\n",
      "Epoch 536 — train loss: 0.7186\n",
      "Epoch 537 — train loss: 0.7517\n",
      "Epoch 538 — train loss: 0.7769\n",
      "Epoch 539 — train loss: 0.8146\n",
      "Epoch 540 — train loss: 0.7716\n",
      "Epoch 541 — train loss: 0.7562\n",
      "Epoch 542 — train loss: 0.7134\n",
      "Epoch 543 — train loss: 0.8963\n",
      "Epoch 544 — train loss: 0.8401\n",
      "Epoch 545 — train loss: 0.7938\n",
      "Epoch 546 — train loss: 0.7450\n",
      "Epoch 547 — train loss: 0.8563\n",
      "Epoch 548 — train loss: 0.8292\n",
      "Epoch 549 — train loss: 0.7913\n",
      "Epoch 550 — train loss: 0.7494\n",
      "Epoch 551 — train loss: 0.7584\n",
      "Epoch 552 — train loss: 0.7250\n",
      "Epoch 553 — train loss: 0.7505\n",
      "Epoch 554 — train loss: 0.7387\n",
      "Epoch 555 — train loss: 0.7119\n",
      "Epoch 556 — train loss: 0.6947\n",
      "Epoch 557 — train loss: 0.6858\n",
      "Epoch 558 — train loss: 0.7230\n",
      "Epoch 559 — train loss: 0.7100\n",
      "Epoch 560 — train loss: 0.7066\n",
      "Epoch 561 — train loss: 0.7105\n",
      "Epoch 562 — train loss: 0.6713\n",
      "Epoch 563 — train loss: 0.6818\n",
      "Epoch 564 — train loss: 0.6565\n",
      "Epoch 565 — train loss: 0.6628\n",
      "Epoch 566 — train loss: 0.7172\n",
      "Epoch 567 — train loss: 0.7076\n",
      "Epoch 568 — train loss: 0.7369\n",
      "Epoch 569 — train loss: 0.7872\n",
      "Epoch 570 — train loss: 0.7036\n",
      "Epoch 571 — train loss: 0.7174\n",
      "Epoch 572 — train loss: 0.7222\n",
      "Epoch 573 — train loss: 0.7485\n",
      "Epoch 574 — train loss: 0.7147\n",
      "Epoch 575 — train loss: 0.7093\n",
      "Epoch 576 — train loss: 0.6805\n",
      "Epoch 577 — train loss: 0.7387\n",
      "Epoch 578 — train loss: 0.6809\n",
      "Epoch 579 — train loss: 0.7011\n",
      "Epoch 580 — train loss: 0.6483\n",
      "Epoch 581 — train loss: 0.7520\n",
      "Epoch 582 — train loss: 0.7542\n",
      "Epoch 583 — train loss: 0.7393\n",
      "Epoch 584 — train loss: 0.7402\n",
      "Epoch 585 — train loss: 0.7264\n",
      "Epoch 586 — train loss: 0.6573\n",
      "Epoch 587 — train loss: 0.6693\n",
      "Epoch 588 — train loss: 0.7078\n",
      "Epoch 589 — train loss: 0.7072\n",
      "Epoch 590 — train loss: 0.6941\n",
      "Epoch 591 — train loss: 0.7194\n",
      "Epoch 592 — train loss: 0.6638\n",
      "Epoch 593 — train loss: 0.7182\n",
      "Epoch 594 — train loss: 0.6712\n",
      "Epoch 595 — train loss: 0.7197\n",
      "Epoch 596 — train loss: 0.7346\n",
      "Epoch 597 — train loss: 0.7306\n",
      "Epoch 598 — train loss: 0.7333\n",
      "Epoch 599 — train loss: 0.6989\n",
      "Epoch 600 — train loss: 0.7067\n"
     ]
    }
   ],
   "source": [
    "# ── 7) Training loop ───────────────────────────────────────────────────────────\n",
    "for epoch in range(1, 601):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        logits = model(xb)\n",
    "        loss   = loss_fn(logits, yb)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * xb.size(0)\n",
    "\n",
    "    avg_loss = running_loss / len(train_ds)\n",
    "    print(f\"Epoch {epoch:02d} — train loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1771ef65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        29\n",
      "           1       0.17      0.64      0.27        25\n",
      "           2       0.57      0.59      0.58       103\n",
      "           3       0.00      0.00      0.00        27\n",
      "           4       0.00      0.00      0.00        16\n",
      "\n",
      "    accuracy                           0.39       200\n",
      "   macro avg       0.15      0.25      0.17       200\n",
      "weighted avg       0.32      0.39      0.33       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ── 8) Evaluation ──────────────────────────────────────────────────────────────\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb = xb.to(device)\n",
    "        logits = model(xb)\n",
    "        preds  = torch.argmax(logits, dim=1).cpu()\n",
    "        all_preds.extend(preds.tolist())\n",
    "        all_labels.extend(yb.tolist())\n",
    "\n",
    "print(classification_report(\n",
    "    all_labels,\n",
    "    all_preds,\n",
    "    digits=2,\n",
    "    zero_division=0\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea9ed01",
   "metadata": {},
   "source": [
    "# BERT Text Classifier\n",
    "Use Pre-trained BERT then fine tune using training data.  Hyperparamters were adjusted to reach best results possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb784499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# ── 1) Device ──────────────────────────────────────────────────────────────────\n",
    "device = torch.device(\"mps\" if torch.mps.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "\n",
    "X_train = train_df[\"bert_text\"]\n",
    "X_test = test_df[\"text\"]\n",
    "\n",
    "y_train = train_df[\"label\"]\n",
    "y_test = test_df[\"label\"]\n",
    "\n",
    "train_texts = X_train.tolist()\n",
    "test_texts  = X_test.tolist()\n",
    "\n",
    "train_labels = y_train.tolist()\n",
    "test_labels  = y_test.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b80c67b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95th percentile train length: 138\n",
      "95th percentile test length:  151\n",
      "96th percentile train length: 144\n",
      "96th percentile test length:  164\n",
      "97th percentile train length: 162\n",
      "97th percentile test length:  178\n",
      "98th percentile train length: 177\n",
      "98th percentile test length:  208\n",
      "99th percentile train length: 246\n",
      "99th percentile test length:  240\n"
     ]
    }
   ],
   "source": [
    "# Get max_length\n",
    "# Tokenize\n",
    "tok_train = [txt.split() for txt in X_train]\n",
    "tok_test  = [txt.split() for txt in X_test]\n",
    "\n",
    "# Compute lengths\n",
    "train_lens = np.array([len(s) for s in tok_train])\n",
    "test_lens  = np.array([len(s) for s in tok_test])\n",
    "\n",
    "# Decide your percentile, e.g. 95%\n",
    "for p in range(95, 100):\n",
    "    max_len_train = int(np.percentile(train_lens, p))\n",
    "    max_len_test  = int(np.percentile(test_lens,  p))\n",
    "\n",
    "    print(f\"{p}th percentile train length: {max_len_train}\")\n",
    "    print(f\"{p}th percentile test length:  {max_len_test}\")\n",
    "\n",
    "max_len = 175  # Set this to the maximum length you want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e58d9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ── 2) Tokenizer & Model ───────────────────────────────────────────────────────\n",
    "PRETRAINED = \"bert-base-cased\"\n",
    "NUM_LABELS = len(train_df[\"label\"].unique())\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED)\n",
    "model     = AutoModelForSequenceClassification.from_pretrained(\n",
    "    PRETRAINED,\n",
    "    num_labels=NUM_LABELS\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6b3472b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 3) Dataset wrapper ─────────────────────────────────────────────────────────\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.texts = texts.tolist()\n",
    "        self.labels = labels.tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        txt  = self.texts[idx]\n",
    "        lab  = self.labels[idx]\n",
    "        enc  = self.tokenizer(\n",
    "            txt,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        # squeeze out the batch dimension\n",
    "        return {\n",
    "            \"input_ids\":      enc[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\":         torch.tensor(lab, dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d56efc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 4) DataLoaders ─────────────────────────────────────────────────────────────\n",
    "BATCH_SIZE = 32\n",
    "MAX_LEN    = 128\n",
    "\n",
    "train_ds = TextDataset(X_train, y_train, tokenizer, max_len=MAX_LEN)\n",
    "test_ds  = TextDataset(X_test,  y_test,  tokenizer, max_len=MAX_LEN)\n",
    "\n",
    "# 1) Count labels\n",
    "counts = Counter(train_labels)            # e.g. {0:50, 1:10, 2:5, ...}\n",
    "\n",
    "# 2) Compute per-example weight = 1 / count[label]\n",
    "example_weights = [1.0 / counts[label] for label in train_labels]\n",
    "example_weights = torch.tensor(example_weights, dtype=torch.double)\n",
    "\n",
    "# 3) Create the sampler\n",
    "sampler = WeightedRandomSampler(\n",
    "    weights=example_weights,    # a list/1D tensor of length N_train\n",
    "    num_samples=len(example_weights),  # draw this many samples per epoch\n",
    "    replacement=True            # sample with replacement\n",
    ")\n",
    "\n",
    "# 4) DataLoader with sampler (no shuffle!)\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sampler=sampler,\n",
    "    drop_last=False\n",
    ")\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c18173bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 5) Optimizer & Scheduler ──────────────────────────────────────────────────\n",
    "EPOCHS     = 6\n",
    "TOTAL_STEPS = len(train_loader) * EPOCHS\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=TOTAL_STEPS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e053a2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 — avg train loss: 1.5598\n",
      "Epoch 2 — avg train loss: 1.0733\n",
      "Epoch 3 — avg train loss: 0.5886\n",
      "Epoch 4 — avg train loss: 0.3821\n",
      "Epoch 5 — avg train loss: 0.2900\n",
      "Epoch 6 — avg train loss: 0.2622\n"
     ]
    }
   ],
   "source": [
    "# ── 6) Training Loop ──────────────────────────────────────────────────────────\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        # move inputs to GPU\n",
    "        input_ids      = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels         = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch} — avg train loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "20da7eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test-set classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.83      0.80        29\n",
      "           1       0.56      0.72      0.63        25\n",
      "           2       0.82      0.79      0.80       103\n",
      "           3       0.57      0.59      0.58        27\n",
      "           4       0.80      0.50      0.62        16\n",
      "\n",
      "    accuracy                           0.73       200\n",
      "   macro avg       0.71      0.69      0.69       200\n",
      "weighted avg       0.75      0.73      0.74       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ── 7) Evaluation ─────────────────────────────────────────────────────────────\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids      = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels         = batch[\"labels\"].to(device)\n",
    "\n",
    "        logits = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        ).logits\n",
    "\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        all_preds.extend(preds.cpu().tolist())\n",
    "        all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "print(\"\\nTest-set classification report:\")\n",
    "print(classification_report(all_labels, all_preds, digits=2, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5753dbb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('bert_model/tokenizer_config.json',\n",
       " 'bert_model/special_tokens_map.json',\n",
       " 'bert_model/vocab.txt',\n",
       " 'bert_model/added_tokens.json',\n",
       " 'bert_model/tokenizer.json')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model and tokenizer\n",
    "model.save_pretrained(\"bert_model/\")\n",
    "tokenizer.save_pretrained(\"bert_model/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tcfd-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
